{
  "name": "crawler-ninja",
  "version": "0.1.0",
  "description": "A web crawler made for the SEO based on plugins. Please wait or test ... still in beta",
  "main": "index.js",
  "repository": {
    "type": "git",
    "url": "https://github.com/christophebe/crawler.ninja"
  },
  "dependencies": {
    "cheerio": "0.18.0",
    "generic-pool": "2.1.1",
    "iconv": "*",
    "iconv-lite": "0.4.4",
    "jschardet": "1.1.0",
    "lodash": "2.4.1",
    "request": "2.42.0",
    "URIjs": "^1.14.2",
    "collections": "*",
    "crypto": "0.0.3",
    "underscore": "^1.8.2"
  },
  "optionalDependencies": {
    "iconv": "*"
  },
  "devDependencies": {
    "mocha": "*",
    "expect": "*",
    "express": "*",
    "serve-static": "*"
  },
  "keywords": [
    "web",
    "crawler",
    "crawler",
    "seo",
    "crawler"
  ],
  "author": {
    "name": "Christophe Lombart"
  },
  "license": "Apache",
  "bugs": {
    "url": "https://github.com/christophebe/crawler.ninja/issues"
  },
  "homepage": "https://github.com/christophebe/crawler.ninja",
  "readme": "Crawler Ninja\n-------------\n\nThis crawler aims to help SEO to build custom solutions for crawling/scraping sites.\nFor example, this crawl can help to audit a site, find expired domains, build corpus, find netlinking spots, retrieve site ranking, check if web pages are correctly indexed, ...\n\nThis is just a matter of plugins ! :-) We plan to build generic & simple plugins but you are free to create your own.\n\nThe best environment to run Crawler Ninja is a linux server.\n\n\nHelp & Forks welcomed ! or please wait ... work in progress !\n\nHow to install\n--------------\n\n    $ npm install crawler-ninja\n\n\nCrash course\n------------\n*How to use an existing plugin ?*\n\n```javascript\nvar crawler = require(\"crawler-ninja\");\nvar logger  = require(\"crawler-ninja/plugins/log-plugin\");\n\nvar c = new crawler.Crawler();\n\nvar log = new logger.Plugin(c);\n\nc.on(\"end\", function() {\n\n    var end = new Date();\n    console.log(\"Well done Sir !, done in : \" + (end - start));\n\n\n});\n\nvar start = new Date();\nc.queue({url : \"http://www.authorize.net/\"});\n```\n\nOptions reference\n-----------------\n\nTODO\n\nCurrent Plugins\n---------------\n\n- Console Log\n- Stat\n- Audit\n\n\n\nRough todolist\n--------------\n\n * Add proxy supports (in progress)\n * More & more plugins\n * Use Riak as default persistence layer\n * Use RabbitMQ\n * Build UI : dashboards, view project data, ...\n\n\nChangeLog\n---------\n\n0.1.0\n - crawler engine that support navigation through a.href, detect images, links tag & scripts.\n - Add flexible parameters to crawl (see in the file : index.js) like the crawl depth, crawl rates, craw external links, ...\n - Implement a basic log plugin & an SEO audit plugin.\n",
  "readmeFilename": "README.md",
  "gitHead": "8ae8cbff589de472aa0f184f346a254aaf0a69fd",
  "_id": "crawler-ninja@0.1.0",
  "scripts": {},
  "_shasum": "f5f9f31f60330611d3cdcb1643af8d8ab39eda27",
  "_from": "crawler-ninja@*"
}
